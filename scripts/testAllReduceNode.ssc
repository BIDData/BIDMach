:silent
import BIDMach.Learner
import BIDMach.allreduce.AllreduceNode.{getBasicConfigs, startNodeAfterIter}
import BIDMach.allreduce.binder.ElasticAverageBinder
import BIDMach.allreduce.AllreduceNode
import BIDMach.datasources.FileSource
import BIDMach.networks.Net
import BIDMach.networks.layers._
import BIDMach.updaters.Grad
import BIDMat.SciFunctions

val traindir = "../data/ImageNet/train/";
//val traindir = "/home/jfc/data/ImageNet/2012/BIDMach/train/";
val testdir = "../data/ImageNet/val/";
val traindata = traindir+"partNCHW%04d.bmat.lz4";
val trainlabels = traindir+"label%04d.imat.lz4";
val testdata = testdir+"partNCHW%04d.bmat.lz4";
val testlabels = testdir+"label%04d.imat.lz4";
val testpreds = testdir+"pred%04d.fmat.lz4";

SciFunctions.setseed(4)

val (nn, opts) = Net.gradLearner(traindata, trainlabels);
val net = nn.model.asInstanceOf[Net]
val ds = nn.datasource.asInstanceOf[FileSource];
val grad = nn.updater.asInstanceOf[Grad]

val alpha = 0.3f
val lrinit = 2e-2f

val epochsecs = 2000f
def lr_update(ipass:Float, istep:Float, frac:Float):Float = {
  if (toc < 20*epochsecs) { 
    lrinit
  } else if (toc < 40*epochsecs) { 
    lrinit/10
  } else if (toc < 60*epochsecs) { 
    lrinit/100
  } else { 
    lrinit/1000
  }
}   

opts.logfile = "logAlexnet_cluster4_lr_%4.3f_alpha_%2.1f.txt" format (lrinit,alpha);
opts.checkPointFile = "../models/alexnet%03d/"
opts.checkPointInterval = 10f;
opts.keepCheckPoints = 2
opts.batchSize= 128;
opts.npasses = 80;
//opts.nend = 10;
opts.texp = 0f;
opts.pstep = 0.05f
opts.hasBias = true;
opts.l2reg = 0.0002f;
opts.vel_decay = 0.9f;
opts.lr_policy = lr_update _;
opts.tensorFormat = Net.TensorNCHW;
opts.useCache = false;
opts.convType = Net.CrossCorrelation;
opts.inplace = Net.BackwardCaching;
opts.inplace = Net.InPlace;

val means = ones(3\256\256\opts.batchSize) *@ loadFMat(traindir+"means.fmat.lz4");

{
  import BIDMach.networks.layers.Node._;

  Net.initDefaultNodeSet;

  val in =        input();
  val meanv =     const(means);
  val din =       in - meanv;
  val scalef =    const(row(0.01f));
  //val sdin =      din *@ scalef;
  //val fin =       format(in)();
  val cin =       cropMirror(din)(sizes=irow(3,227,227,0), randoffsets=irow(0,28,28,-1));
  //val min =       randmirror(cin)();

  val conv1 =     conv(cin)(w=11,h=11,nch=96,stride=4,initfn=Net.gaussian,initv=0.01f,initbiasv=0f);
  val relu1 =     relu(conv1)();
  val pool1 =     pool(relu1)(w=3,h=3,stride=2);
  val norm1 =     LRNacross(pool1)(dim=5,alpha=0.0001f,beta=0.75f);

  val conv2 =     conv(norm1)(w=5,h=5,nch=256,stride=1,pad=2,initfn=Net.gaussian,initv=0.01f,initbiasv=1f);
  val relu2 =     relu(conv2)();
  val pool2 =     pool(relu2)(w=3,h=3,stride=2);
  val norm2 =     LRNacross(pool2)(dim=5,alpha=0.0001f,beta=0.75f);

  val conv3 =     conv(norm2)(w=3,h=3,nch=384,pad=1,initfn=Net.gaussian,initv=0.01f,initbiasv=0f);
  val relu3 =     relu(conv3)();

  val conv4 =     conv(relu3)(w=3,h=3,nch=384,pad=1,initfn=Net.gaussian,initv=0.01f,initbiasv=1f);
  val relu4 =     relu(conv4)();

  val conv5 =     conv(relu4)(w=3,h=3,nch=256,pad=1,initfn=Net.gaussian,initv=0.01f,initbiasv=1f);
  val relu5 =     relu(conv5)();
  val pool5 =     pool(relu5)(w=3,h=3,stride=2);

  val fc6 =       linear(pool5)(outdim=4096,initfn=Net.gaussian,initv=0.01f,initbiasv=1f);
  val relu6 =     relu(fc6)();
  val drop6 =     dropout(relu6)(0.5f);

  val fc7 =       linear(drop6)(outdim=4096,initfn=Net.gaussian,initv=0.01f,initbiasv=1f);
  val relu7  =    relu(fc7)();
  val drop7 =     dropout(relu7)(0.5f);

  val fc8  =      linear(drop7)(outdim=1000,initfn=Net.gaussian,initv=0.01f,initbiasv=1f);
  val out =       softmaxout(fc8)(scoreType=1,lossType=1);

  opts.nodeset=Net.getDefaultNodeSet
}
def loss = {net.layers(net.layers.length-1).asInstanceOf[SoftmaxOutputLayer]};

val sgd = nn.updater.asInstanceOf[Grad];



nn.launchTrain

// All-reduce config
import BIDMach.allreduce.{ThresholdConfig,MetaDataConfig,WorkerConfig,LineMasterConfig,NodeConfig}

val dimNum = 2
val maxChunkSize = 20000
val roundWorkerPerDimNum = 3
val maxRound = 1000000

val threshold = ThresholdConfig(thAllreduce = 1f, thReduce = 1f, thComplete = 1f)
val metaData = MetaDataConfig(maxChunkSize = maxChunkSize)

val workerConfig = WorkerConfig(
  statsReportingRoundFrequency = 5,
  threshold = threshold,
  metaData = metaData)

val lineMasterConfig = LineMasterConfig(
  roundWorkerPerDimNum = roundWorkerPerDimNum,
  dim = -1,
  maxRound = maxRound,
  workerResolutionTimeout = 10.seconds,
  threshold = threshold)

val nodeConfig = NodeConfig(workerConfig, lineMasterConfig, dimNum = dimNum, reportStats = true, elasticRate = alpha)

val binder = new ElasticAverageBinder(nn.model, _ => nodeConfig.elasticRate, nn.myLogger)

AllreduceNode.startNodeAfterIter(nn, iter = 0, nodeConfig, binder)

println("Examine the 'nn' variable to track learning state.\n");

//nn.train;

//val (mm, mopts) =  Net.predLabels(net, testdata, testlabels);
//mopts.batchSize= opts.batchSize;
//mopts.autoReset = false;
//mm.predict;

//println("Accuracy = %f" format mean(mm.results(0,?),2).v);



