:silent
import BIDMach.models.SMF

/**
 * Test SMF code on netflix data. This will use OUR MHTest updater, which I put
 * in as a new updater (SMF.learner2) to make this script more concise. Some
 * notes on the netflix dataset:
 *
 * size(a) = (17770,480189)
 * a.nnz = 90430138
 * min=0, max=5
 * 
 * (a == 1).nnz = 4156151
 * (a == 2).nnz = 9120198
 * (a == 3).nnz = 25928920
 * (a == 4).nnz = 30375037
 * (a == 5).nnz = 20849832
 * mean (of nonzeros) = 3.6042476
 * sqrt((diff ddot diff) / diff.nn) = 1.0852 // Train RMSE using mean predictor
 *
 * (ta == 1).nnz = 461839
 * (ta == 2).nnz = 1011882
 * (ta == 3).nnz = 2882327
 * (ta == 4).nnz = 3375921
 * (ta == 5).nnz = 2318400
 * mean (of nonzeros) = 3.6046705
 * sqrt((diff ddot diff) / diff.nn) = 1.0851 // Test RMSE using mean predictor
 *
 * BTW: (a *@ ta).nnz = 0, which shows that they are completely distinct.
 */

// Get random seed set up.
// TODO random seed code

// Now get back to the real netflix data. First, load data and set things up:
val dir = "/data/netflix/"
val a = loadSMat(dir+"newtrain.smat.lz4")
val ta = loadSMat(dir+"newtest.smat.lz4")
val d = 256
val (nn,opts) = SMF.learner2(a, d)
println("size(a)="+size(a)+", with a.nnz="+a.nnz)

// Daniel Seita: stuff for the MH Test updater. OH ... and our N is going to be
// super-large. Ahhhh... we may need temperature then.
opts.N = a.nnz
opts.temp = a.nnz / 10000
opts.Nknown = true
opts.n2lsigma = 1.0f
opts.nn2l = 4000
opts.sigmaProposer = 0.01f
opts.continueDespiteFull = false
opts.verboseMH = false
opts.collectData = false
opts.collectDataDir = "tmp/"
opts.exitTheta = false
opts.initThetaHere = true
opts.burnIn = -1

// IMPORTANT. setting opts.smf=true means we create an ADAGrad class inside.
// We also want to compare with additive random noise, i.e. Langevin stuff.
opts.smf = true
opts.langevin = 0.0f
opts.momentum = FMat(0.5)
opts.nesterov = null
opts.saveAcceptRate = true

// Daniel Seita: actually, a batch size of 2000 means we may get 300k "elements"
// due to the sparsity. So I'm thinking we stick to batch sizes of 1000 or less.
// Also, the four items here that start with `lambda` or `reg` represent priors.
opts.matrixOfScores = true
opts.batchSize = 1000
opts.npasses = 3
opts.uiter = 5
opts.urate = 0.05f
opts.lrate = 0.05f  
val lambda = 4f
opts.lambdau = lambda
opts.regumean = lambda
opts.lambdam = lambda / 500000 * 20
opts.regmmean = opts.lambdam
opts.evalStep = 31
opts.doUsers = false
opts.what
nn.train

val model = nn.model.asInstanceOf[SMF]
val xa = (ta != 0)
val (mm, mopts) = SMF.predictor1(model, a, xa)

mopts.batchSize = 10000
mopts.uiter = 5
mopts.urate = opts.urate
mopts.lsgd = 0.0f
mm.predict

val pa = SMat(mm.preds(1));
println("Note: max(pa)="+maxi(maxi(pa))+" and min(pa)="+mini(mini(pa)))
val diff = ta.contents - pa.contents
val rmse = sqrt((diff ddot diff) / diff.length)
println("rmse = %f" format rmse.v)
min(pa.contents,5,pa.contents)
max(pa.contents,1,pa.contents)
val diff2 = ta.contents - pa.contents
val rmse2 = sqrt((diff2 ddot diff2) / diff2.length)
println("rmse (w/clipping) = %f" format rmse2.v)
