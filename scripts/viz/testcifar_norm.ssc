:silent

import jcuda.jcudnn._

val datadir = "/data/mcmc/data/CIFAR10/parts/"
val pretrain_model_dir = "models/cifar_new/"
val pretrain_discriminator_dir = "models/cifar_discri2/"

val trainfname = datadir + "trainNCHW%d.fmat.lz4";
val labelsfname = datadir + "labels%d.imat.lz4";
val testfname = datadir + "testNCHW%d.fmat.lz4";
val testlabelsfname = datadir + "testlabels%d.imat.lz4";
val predsfname = datadir + "preds%d.fmat.lz4";

val (nn,opts) = Net.learner(trainfname,labelsfname);

val convt = jcuda.jcudnn.cudnnConvolutionMode.CUDNN_CROSS_CORRELATION


opts.batchSize= 100
opts.npasses = 1000
opts.lrate = 1e-4f 

opts.vel_decay = 0.9f
opts.gsq_decay = 0.99f
opts.texp = 0.0f
opts.pstep = 0.1f
opts.hasBias = true;
opts.tensorFormat = Net.TensorNCHW;
opts.autoReset = false;
opts.debugMem = false;

/***val means = loadFMat(trainfname format 0).mean(irow(3))
{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val meanv = const(means);
    val din = in - meanv;
    val scalef = constant(row(0.01f));
    val inscale = din *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=0,initv=1f,convType=convt);
    val pool1 = pool(conv1)(w=2,h=2,stride=2);
    //val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=0,convType=convt);
    val pool2 = pool(conv2)(w=2,h=2,stride=2);
    //val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2  )();

    val conv3 = conv(relu2)(w=5,h=5,nch=32,stride=1,pad=2,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2);
    val relu3 = relu(pool3)();
    val fc3 =   linear(relu3)(outdim=10,initv=3e-2f);
    val out =   softmaxout(fc3)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}*/


val means = loadFMat(trainfname format 0).mean(irow(3))
{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val meanv = const(means);
    val din = in - meanv;
    val scalef = constant(row(0.01f));
    val inscale = din *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=64,stride=1,pad=0,initv=1f,convType=convt);
    val pool1 = pool(conv1)(w=3,h=3,stride=2);
    //val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=64,stride=1,pad=0,convType=convt);
    val pool2 = pool(conv2)(w=2,h=2,stride=2);
    //val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2)();

    val fc3 =   linear(relu2)(outdim=384,initv=3e-2f);
    val relu3 = relu(fc3)();
    val fc4 =   linear(relu3)(outdim=192,initv=3e-2f);
    val relu4 = relu(fc4)();
    val fc5 =   linear(relu4)(outdim=10,initv=3e-2f);
    val out =   softmaxout(fc5)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}


/**{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.01f,convType=convt);
    val pool1 = pool(conv1)(w=3,h=3,stride=2);
//    val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool2 = pool(conv2)(w=3,h=3,stride=2)//,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2)();

    val conv3 = conv(relu2)(w=5,h=5,nch=64,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2)//,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val fc3 =   linear(pool3)(outdim=64,initv=1e-1f);
//    val relu3 = relu(fc3)();
    val fc4 =   linear(pool3)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc4)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}*/

/**{
    import BIDMach.networks.layers.Node._;
    import BIDMach.networks.layers.NodeTerm
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef
    
    var layer:NodeTerm = inscale
    val config = List(64, 64, 0, 128, 128, 0, 256, 256, 256, 0, 512, 512, 512, 0, 512, 512, 512, 0)
    
    for(i<-config){
        if (i == 0) {
            layer = pool(layer)(w=2,h=2,stride=2)
        }
        else {
            layer = conv(layer)(w=3,h=3,nch=i,stride=1,pad=1,initv=0.01f,convType=convt);
            layer = batchNormScale(layer)();
            layer = relu(layer)();
        }
    }

    val fc =   linear(layer)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet
}*/

//opts.nodemat = nodes;
val model = nn.model.asInstanceOf[Net];
nn.launchTrain;

import BIDMach.viz._
import BIDMach.networks.layers._
//val v = nn.add_plot(new FilterViz(3,bw=5)).asInstanceOf[FilterViz]
//v.interval = 100

def load(net:Net,fname:String) {
    for (i <- 0 until net.modelmats.length) {
        val data = loadMat(fname+"modelmat%02d.lz4" format i);
        net.modelmats(i)<--data
    }
}

nn.pause
Thread.sleep(1000)
val s = new Synthesis("cifar")
val o = s.opts
load(model,pretrain_model_dir)
o.endLayer = -2
//o.guidebp = false
o.realImagesPath = datadir;
o.pretrainedDiscriminatorPath = pretrain_discriminator_dir //"models/cifar_discri2/"
//o.wClip = 0.01f
s.init(model,null)
//o.base = 0
//o.scale = 1
o.iter=30
o.langevin = 0
//o.dWeight=0.5f
s.mcmc(model)
val ii = IMat(FMat(model.layers(model.layers.length-1).target + irow(0->100)*10))
o.derivFunc = (a:Layer)=>{val m = a.deriv;m.set(0f);m(ii)=1f}
o.printInfo = false
o.clipping = true

:silent

def reset() {
    val ii = IMat(FMat(model.layers(model.layers.length-1).target + irow(0->100)*10))
    o.derivFunc = (a:Layer)=>{val m = a.deriv;m.set(0f);m(ii)=1f}
    o.endLayer = -2;
}

def setConv() {
    val p = zeros(128,32)
    for(i<-0 until 32)
        p((4*i)->(4*i+4),i) = 1f
    val id = irow(IMat(sortdown2((model.modelmats(6) * p).t)._2(0->10,?).t).data)
    s.opts.derivFunc = (a:Layer)=>{
        val m = a.deriv;m.set(0f);
        var ind = id + irow(0->m.dims(3)) * m.dims(0)
        val rm = m.reshapeView(m.dims(1),m.dims(2),m.dims(0)*m.dims(3));
        rm(m.dims(1)/2,m.dims(2)/2,ind(0->m.dims(3)/2))=1f;
        rm(m.dims(1)/2,m.dims(2)/2,ind((m.dims(3)/2)->m.dims(3)))=1f;
//                                      rm(m.dims(1)/2,m.dims(2)/2,0,)=1f
        //Weird bug, 4d slice for GMat can't support bigger than 64 items if the last index is ?
    }
    //val m = col(FMat(model.modelmats(6)).data.grouped(4).map(_.sum).toArray)
}



//var threhold=1;
//hist(()=>row(FMat(model.layers(0).output).data.filter(math.abs(_)>threhold)),100)

//:load gan.ssc
//val s = start("cifar")
//plotscore(s.dscores)
//plotscore(s.gscores)
//plotscore(s.gsteps)
//s.lrate=0.1f
