:silent

val datadir = "/commuter/CIFAR10/parts/"
val trainfname = datadir + "trainNCHW%d.fmat.lz4";
val labelsfname = datadir + "labels%d.imat.lz4";
val testfname = datadir + "testNCHW%d.fmat.lz4";
val testlabelsfname = datadir + "testlabels%d.imat.lz4";
val predsfname = datadir + "preds%d.fmat.lz4";

val (nn,opts) = Net.learner(trainfname,labelsfname);

val convt = jcuda.jcudnn.cudnnConvolutionMode.CUDNN_CROSS_CORRELATION
import jcuda.jcudnn._

opts.batchSize= 100
opts.npasses = 10
opts.lrate = 1e-4f 

opts.vel_decay = 0.9f
opts.gsq_decay = 0.99f
opts.texp = 0.0f
opts.pstep = 0.1f
opts.hasBias = true;
opts.tensorFormat = Net.TensorNCHW;
opts.autoReset = false;
opts.debugMem = false;

/*{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=0,initv=1f,convType=convt);
    val pool1 = pool(conv1)(w=2,h=2,stride=2);
    //val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=0,convType=convt);
    val pool2 = pool(conv2)(w=2,h=2,stride=2);
    //val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2)();

    val conv3 = conv(relu2)(w=5,h=5,nch=32,stride=1,pad=2,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2);
    val fc3 =   linear(pool3)(outdim=10,initv=3e-2f);
    val out =   softmaxout(fc3)(scoreType=1); 

    /*(in     \ scalef \ inscale on
    conv1  \ pool1  \ relu1  on
    conv2  \ pool2  \ relu2  on
    conv3  \ pool3  \ null   on
    fc3    \ out    \ null   ).t;*/
    opts.nodeset=Net.getDefaultNodeSet

}*/

/*{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.01f,convType=convt);
    val pool1 = pool(conv1)(w=3,h=3,stride=2);
    val norm1 = batchNormScale(pool1)();
    val relu1 = relu(norm1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool2 = pool(conv2)(w=3,h=3,stride=2,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
    val norm2 = batchNormScale(pool2)();
    val relu2 = relu(norm2)();

    val conv3 = conv(relu2)(w=5,h=5,nch=64,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val fc3 =   linear(pool3)(outdim=64,initv=1e-1f);
//    val relu3 = relu(fc3)();
    val fc4 =   linear(pool3)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc4)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}*/

{
    import BIDMach.networks.layers.Node._;
    import BIDMach.networks.layers.NodeTerm
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef
    
    var layer:NodeTerm = inscale
    val config = List(64, 64, 0, 128, 128, 0, 256, 256, 256, 0, 512, 512, 512, 0, 512, 512, 512, 0)
    
    for(i<-config){
        if (i == 0) {
            layer = pool(layer)(w=2,h=2,stride=2)
        }
        else {
            layer = conv(layer)(w=3,h=3,nch=i,stride=1,pad=1,initv=0.01f,convType=convt);
            layer = batchNormScale(layer)();
            layer = relu(layer)();
        }
    }

    val fc =   linear(layer)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet
}




//opts.nodemat = nodes;
val model = nn.model.asInstanceOf[Net];
nn.launchTrain;

import BIDMach.viz._
import BIDMach.networks.layers._
//val v = nn.add_plot(new FilterViz(3,bw=5)).asInstanceOf[FilterViz]
//v.interval = 100

def load(net:Net,fname:String) {
    for (i <- 0 until net.modelmats.length) {
        val data = loadMat(fname+"modelmat%02d.lz4" format i);
        net.modelmats(i)<--data
    }
}

nn.pause
Thread.sleep(1000)


def reset() {
    val ii = IMat(FMat(model.layers(12).target + irow(0->100)*10))
//    o.derivFunc = (a:Layer)=>{val m = a.deriv;m.set(0f);m(ii)=1f}
//    o.endLayer = 11;
}

var threhold=1;

val test =         loadFMat(testfname format 0)
val testlabels =   IMat(loadFMat(testlabelsfname format 0))
val (mm, mopts) =  Net.predictor(model, test, testlabels);

def test() = {
    mopts.pstep = 1f
    mopts.autoReset = false;
    val mmodel = mm.model.asInstanceOf[Net];
    for(i<-0 until mmodel.modelmats.length) {
        mmodel.modelmats(i)<--model.modelmats(i)
    }
    mm.predict;
//    println("Accuracy = %f" format mean(mm.results(0,?),2).v);
    mean(mm.results(0,?),2).v
}

val updater = nn.updater
val ds = nn.datasource
val net = model
val _averagingModelmats = new Array[Mat](net.modelmats.length);
val tmp = new Array[Mat](net.modelmats.length);
for(i<-0 until net.modelmats.length) {
    _averagingModelmats(i) = net.modelmats(i).zeros(net.modelmats(i).dims)
    tmp(i) = net.modelmats(i).zeros(net.modelmats(i).dims)
    _averagingModelmats(i) <-- net.modelmats(i)
}
val _averagingWeight = net.modelmats(0).zeros(1,1);

def test_avg() = {
    mopts.pstep = 1f
    mopts.autoReset = false;

    val mmodel = mm.model.asInstanceOf[Net];
    for(i<-0 until mmodel.modelmats.length) {
        tmp(i)<--mmodel.modelmats(i)
        mmodel.modelmats(i)<--_averagingModelmats(i)
    }
    mm.predict;
    for(i<-0 until mmodel.modelmats.length) {
        mmodel.modelmats(i)<--tmp(i)
    }
    //println("Accuracy = %f" format mean(mm.results(0,?),2).v);
    mean(mm.results(0,?),2).v
}

def train_merge(interval: Int = 1,rate:Float = 0.99f) = {
    var s = 0f
    var s2 = 0f
    val total = 5
    for(tt<-0 until total){
        load(net,"models/cifar_vgg/")
        for(i<-0 until net.modelmats.length) {
            _averagingModelmats(i) <-- net.modelmats(i)
        }
        for(t<-0 until 5){
            ds.reset
            var here = 0
            while (ds.hasNext){
                net.dobatchg(ds.next,t,here);
                updater.update(t,here,0);
                here += ds.opts.batchSize
                if (here/ds.opts.batchSize % interval == 0)
                    for(i<-0 until net.modelmats.length){
                        _averagingWeight(0,0) = rate
                        _averagingModelmats(i) ~ _averagingModelmats(i) *@ _averagingWeight;
                        _averagingModelmats(i) ~ _averagingModelmats(i) + (net.modelmats(i) *@ (1f - _averagingWeight))
                    }
            }
            //println(mean(net.output_layers(0).score))
            if (t% 10 ==4 || t% 10 == 9) {
//                println("iter: "+ (t+1))
                s += test()
                s2 += test_avg()
                }
        }
    }
    (s/total,s2/total)
}

val f = List( 1,2,5,  1, 5,10,  1,10,50,100,   1,  2, 10, 50,100,200,500)
val t = List(10,5,2,100,20,10,500,50,10,  5,1000,500,500,100, 50, 25, 10)
//val ff = List(1,1)
//val tt = List(100,500)
import scala.collection.mutable.ListBuffer
val reg = new ListBuffer[Float]
val avg = new ListBuffer[Float]
for(i<-f.length-5 until f.length){
//for(i<-0 until ff.length){
        val interval = f(i)
        val rate = 1-1f/t(i)
        println("Interval: "+interval + "   Rate: "+rate)
        val (s,s2) = train_merge(interval,rate)
        println(s,s2)
        reg.append(s)
        avg.append(s2)
    }

