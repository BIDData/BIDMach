:silent

import jcuda.jcudnn._

val datadir = "/data/data/CIFAR10/parts/"
val trainfname = datadir + "trainNCHW%d.fmat.lz4";
val labelsfname = datadir + "labels%d.imat.lz4";
val testfname = datadir + "testNCHW%d.fmat.lz4";
val testlabelsfname = datadir + "testlabels%d.imat.lz4";
val predsfname = datadir + "preds%d.fmat.lz4";

val (nn,opts) = Net.learner(trainfname,labelsfname);

val convt = jcuda.jcudnn.cudnnConvolutionMode.CUDNN_CROSS_CORRELATION


opts.batchSize= 100
opts.npasses = 1000
opts.lrate = 1e-4f 

opts.vel_decay = 0.9f
opts.gsq_decay = 0.99f
opts.texp = 0.0f
opts.pstep = 0.1f
opts.hasBias = true;
opts.tensorFormat = Net.TensorNCHW;
opts.autoReset = false;
opts.debugMem = false;

/**{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=0,initv=1f,convType=convt);
    val pool1 = pool(conv1)(w=2,h=2,stride=2);
    //val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=0,convType=convt);
    val pool2 = pool(conv2)(w=2,h=2,stride=2);
    //val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2)();

    val conv3 = conv(relu2)(w=5,h=5,nch=32,stride=1,pad=2,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2);
    val fc3 =   linear(pool3)(outdim=10,initv=3e-2f);
    val out =   softmaxout(fc3)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}*/

/**{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.01f,convType=convt);
    val pool1 = pool(conv1)(w=3,h=3,stride=2);
//    val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool2 = pool(conv2)(w=3,h=3,stride=2,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2)();

    val conv3 = conv(relu2)(w=5,h=5,nch=64,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val fc3 =   linear(pool3)(outdim=64,initv=1e-1f);
//    val relu3 = relu(fc3)();
    val fc4 =   linear(pool3)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc4)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}*/

{
    import BIDMach.networks.layers.Node._;
    import BIDMach.networks.layers.NodeTerm
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef
    
    var layer:NodeTerm = inscale
    val config = List(64, 64, 0, 128, 128, 0, 256, 256, 256, 0, 512, 512, 512, 0, 512, 512, 512, 0)
    
    for(i<-config){
        if (i == 0) {
            layer = pool(layer)(w=2,h=2,stride=2)
        }
        else {
            layer = conv(layer)(w=3,h=3,nch=i,stride=1,pad=1,initv=0.01f,convType=convt);
            layer = batchNormScale(layer)();
            layer = relu(layer)();
        }
    }

    val fc =   linear(layer)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet
}


//opts.nodemat = nodes;
val model = nn.model.asInstanceOf[Net];
nn.launchTrain;

import BIDMach.viz._
import BIDMach.networks.layers._
//val v = nn.add_plot(new FilterViz(3,bw=5)).asInstanceOf[FilterViz]
//v.interval = 100

def load(net:Net,fname:String) {
    for (i <- 0 until net.modelmats.length) {
        val data = loadMat(fname+"modelmat%02d.lz4" format i);
        net.modelmats(i)<--data
    }
}

nn.pause
Thread.sleep(1000)
val s = new Synthesis("cifar")
val o = s.opts
load(model,"models/cifar_vgg/")
o.endLayer = -2
o.guidebp = false
o.realImagesPath = "/data/data/CIFAR10/parts/"
o.pretrainedDiscriminatorPath = "models/cifar_discri2/"
//o.wClip = 0.01f
s.init(model,null)
//o.base = 0
//o.scale = 1
o.iter=4
//o.langevin = 0
//o.dWeight=0.5f
s.mcmc(model)
val ii = IMat(FMat(model.layers(model.layers.length-1).target + irow(0->100)*10))
o.derivFunc = (a:Layer)=>{val m = a.deriv;m.set(0f);m(ii)=1f}
o.printInfo = false

:silent

def reset() {
    val ii = IMat(FMat(model.layers(12).target + irow(0->100)*10))
    o.derivFunc = (a:Layer)=>{val m = a.deriv;m.set(0f);m(ii)=1f}
    o.endLayer = 11;
}

var threhold=1;
//hist(()=>row(FMat(model.layers(0).output).data.filter(math.abs(_)>threhold)),100)

//:load gan.ssc
//val s = start("cifar")
//plotscore(s.dscores)
//plotscore(s.gscores)
//plotscore(s.gsteps)
//s.lrate=0.1f
