:silent

import jcuda.jcudnn._
val datadir = "/commuter/CIFAR10/parts/"
val trainfname = datadir + "trainNCHW%d.fmat.lz4";
val labelsfname = datadir + "labels%d.imat.lz4";
val testfname = datadir + "testNCHW%d.fmat.lz4";
val testlabelsfname = datadir + "testlabels%d.imat.lz4";
val predsfname = datadir + "preds%d.fmat.lz4";

class FDSopts extends Learner.Options with Net.Opts with FileSource.Opts with Grad.Opts with L2Regularizer.Opts;


def learner2(fnames:List[(Int)=>String]):(Learner, FDSopts) = {
    val opts = new FDSopts;
    opts.fnames = fnames
    opts.batchSize = 100000;
    opts.eltsPerSample = 500;
    val ds = new FileSource(opts)
    val nn = new Learner(
        ds,
        new Net(opts),
        Array(new L2Regularizer(opts)),
        new Grad(opts),
        null,
        opts)
    (nn, opts)
}

def learner(fn1:String, fn2:String):(Learner, FDSopts) = learner2(List(FileSource.simpleEnum(fn1,1,0),
                                                                    FileSource.simpleEnum(fn2,1,0)));


val (nn,opts) = learner(trainfname,labelsfname);

val convt = jcuda.jcudnn.cudnnConvolutionMode.CUDNN_CROSS_CORRELATION


opts.batchSize= 100
opts.npasses = 100
opts.lrate = 1e-3f 

opts.vel_decay = 0.9f
//opts.gsq_decay = 0.99f
opts.texp = 0.1f
opts.pstep = 0.1f
opts.hasBias = true;
opts.tensorFormat = Net.TensorNCHW;
opts.autoReset = false;
opts.debugMem = false;
opts.reg2weight = 5e-4f

/*{
    import BIDMach.networks.layers.Node._;
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef

    val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.01f,convType=convt);
    val pool1 = pool(conv1)(w=3,h=3,stride=2);
//    val norm1 = batchNormScale(pool1)();
    val relu1 = relu(pool1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool2 = pool(conv2)(w=3,h=3,stride=2,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val norm2 = batchNormScale(pool2)();
    val relu2 = relu(pool2)();

    val conv3 = conv(relu2)(w=5,h=5,nch=64,stride=1,pad=2,initv=0.1f,convType=convt);
    val pool3 = pool(conv3)(w=3,h=3,stride=2,poolingMode=cudnnPoolingMode.CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING);
//    val fc3 =   linear(pool3)(outdim=64,initv=1e-1f);
//    val relu3 = relu(fc3)();
    val fc4 =   linear(pool3)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc4)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet

}*/


{
    import BIDMach.networks.layers.Node._;
    import BIDMach.networks.layers.NodeTerm
    Net.initDefaultNodeSet;

    val in = input;
    val scalef = constant(row(0.01f));
    val inscale = in *@ scalef
    
    var layer:NodeTerm = inscale
    val config = List(64, 64, 0, 128, 128, 0, 256, 256, 256, 0, 512, 512, 512, 0, 512, 512, 512, 0)
    
    for(i<-config){
        if (i == 0) {
            layer = pool(layer)(w=2,h=2,stride=2)
        }
        else {
            layer = conv(layer)(w=3,h=3,nch=i,stride=1,pad=1,initv=0.01f,convType=convt);
            layer = batchNormScale(layer)();
            layer = relu(layer)();
        }
    }

    val fc =   linear(layer)(outdim=10,initv=1e-1f);
    val out =   softmaxout(fc)(scoreType=1); 

    opts.nodeset=Net.getDefaultNodeSet
}

val model = nn.model.asInstanceOf[Net];


def test(){
    val test =         loadFMat(testfname format 0)
    val testlabels =   IMat(loadFMat(testlabelsfname format 0))
    val (mm, mopts) =  Net.predictor(model, test, testlabels);
    mopts.pstep = 1f
    mopts.autoReset = false;

    val mmodel = mm.model.asInstanceOf[Net];
    mm.predict;

    println("Accuracy = %f" format mean(mm.results(0,?),2).v);
}


//opts.nodemat = nodes;
nn.launchTrain;


