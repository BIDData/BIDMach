:silent
val traindir = "/commuter/MNIST/"

class MyOpts extends Net.Opts with MatSource.Opts with ADAGrad.Opts;
val train0 = loadIDX(traindir+"train-images-idx3-ubyte").reshapeView(1,28,28,60000);
val trainlabels0 = loadIDX(traindir+"train-labels-idx1-ubyte").reshapeView(1,60000);
val opts = new MyOpts;
val ds = new MatSource(Array(train0, trainlabels0), opts);
val updater = new ADAGrad(opts);
opts.batchSize = 100;
opts.hasBias = true;
opts.tensorFormat = Net.TensorNCHW;
opts.convType = Net.CrossCorrelation;
opts.lrate = 1e-4f;
opts.vel_decay = 0.9f;
opts.gsq_decay = 0.99f;
opts.texp = 0.1f
Mat.useCache = true;
Mat.useGPUcache = true;

val net = new Net(opts);
{
    import BIDMach.networks.layers.Node._;
    val convt = jcuda.jcudnn.cudnnConvolutionMode.CUDNN_CROSS_CORRELATION
    Net.initDefaultNodeSet;
    val in = input;
    val conv1 = conv(in)(w=5,h=5,nch=6,stride=1,pad=2,convType=convt,initfn=Net.gaussian, initv=0.1f,initbiasv=0.1f);
    val pool1 = pool(conv1)(w=2,h=2,stride=2,pad=0);
    val bns1 = batchNormScale(pool1)();
    val relu1 = relu(bns1)();

    val conv2 = conv(relu1)(w=5,h=5,nch=16,stride=1,pad=2,convType=convt,initfn=Net.gaussian, initv=0.1f,initbiasv=0.1f);   
    val pool2 = pool(conv2)(w=2,h=2,stride=2,pad=0);
    val bns2 = batchNormScale(pool2)();
    val relu2 = relu(bns2)();

    val fc3 = linear(relu2)(outdim=120,initfn=Net.gaussian, initv=0.1f,initbiasv=0.1f);
    val relu3 = relu(fc3)();

    val fc4 = linear(relu3)(outdim=84,initfn=Net.gaussian, initv=0.1f,initbiasv=0.1f);
    val relu4  = relu(fc4)();

    val fc5  = linear(relu4)(outdim=11,initfn=Net.gaussian, initv=0.1f,initbiasv=0.1f);
    val out = softmaxout(fc5)(scoreType=1);
    opts.nodeset=Net.getDefaultNodeSet
}        
ds.init
net.bind(ds)
net.init;
updater.init(net)
(net,updater)


def train(){
    for(i<-0 until 1){
        ds.reset
        var here = 0
        while (ds.hasNext){
            net.dobatchg(ds.next,i,here);
            updater.update(i,here,0);
            here += ds.opts.batchSize        
        }
        println(mean(net.output_layers(0).score))
    }
}

def test(){
    val test =         loadIDX(traindir+"t10k-images-idx3-ubyte").reshapeView(1,28,28,10000);
    val testlabels =   loadIDX(traindir+"t10k-labels-idx1-ubyte").reshapeView(1,10000);
    val (mm, mopts) =  Net.predictor(net, test, testlabels);
    mopts.pstep = 1f
    mopts.autoReset = false;

    val mmodel = mm.model.asInstanceOf[Net];
    mm.predict;

    println("Accuracy = %f" format mean(mm.results(0,?),2).v);
}

val _averagingModelmats = new Array[Mat](net.modelmats.length);
for(i<-0 until net.modelmats.length) {
    _averagingModelmats(i) = net.modelmats(i).zeros(net.modelmats(i).dims)
    _averagingModelmats(i) <-- net.modelmats(i)
}
val _averagingWeight = net.modelmats(0).zeros(1,1);

def train_merge(){
    for(t<-0 until 2){
        ds.reset
        var here = 0
        while (ds.hasNext){
            net.dobatchg(ds.next,t,here);
            updater.update(t,here,0);
            here += ds.opts.batchSize
//            if (here/ds.opts.batchSize % 20 == 0)
                for(i<-0 until net.modelmats.length){
                    _averagingWeight(0,0) = 0.99f
                    _averagingModelmats(i) ~ _averagingModelmats(i) *@ _averagingWeight;
                    _averagingModelmats(i) ~ _averagingModelmats(i) + (net.modelmats(i) *@ (1f - _averagingWeight))
                }
        }
        println(mean(net.output_layers(0).score))
    }
}

def test_avg(){
    val test =         loadIDX(traindir+"t10k-images-idx3-ubyte").reshapeView(1,28,28,10000);
    val testlabels =   loadIDX(traindir+"t10k-labels-idx1-ubyte").reshapeView(1,10000);
    val (mm, mopts) =  Net.predictor(net, test, testlabels);
    mopts.pstep = 1f
    mopts.autoReset = false;

    val mmodel = mm.model.asInstanceOf[Net];
    for(i<-0 until mmodel.modelmats.length)
        mmodel.modelmats(i)<--_averagingModelmats(i)
    mm.predict;

    println("Accuracy = %f" format mean(mm.results(0,?),2).v);
}


train()
test()
train_merge()
test()
test_avg();



:load save.ssc

import BIDMach.viz._
import BIDMach.networks.layers.Layer
val s = new Synthesis("mnist")
val o = s.opts
o.realImagesPath = "/commuter/MNIST/";
s.init(net,null);
o.dWeight = 0f;
o.iter = 30;
o.printInfo = false
o.resetInterval = 100
o.pretrainedDiscriminatorPath = "models/mnist_discri2/"
s.mcmc(net)

val ii = IMat(FMat(net.layers(14).target + irow(0->100)*10))
//o.derivFunc = (a:Layer)=>{val m = a.deriv;m.set(0f);m(ii)=1f}
//o.endLayer = 13
var done = false
def doUpdate(){

    for(i<-0 until 1){
        ds.reset
        var here = 0
        while (ds.hasNext && !done){
            net.dobatchg(ds.next,i,here);
            updater.update(i,here,0);
            here += ds.opts.batchSize
            val score = mean(net.output_layers(0).score)
            val data = s.mcmc(net)
    //        net.layers(0).output<-- data(?,?,?,0->(batchSize/2))
            net.output_layers(0).target(?) = 10
            net.layers(0).deriv.clear;
            net.forward;net.setderiv();net.backward(i, here);
            updater.update(i,here,0);
            here += ds.opts.batchSize
            if (here/ds.opts.batchSize % 10 == 0) 
                s.show(data)
            if (here/ds.opts.batchSize % 100 == 0) {
                println(score, net.layers(14).output(0),net.layers(14).output(10))
            }
              
        }
        println(mean(net.output_layers(0).score))
    }
//    val data = s.mcmc(net)
//    s.show(data)
}
import scala.concurrent.Future;
import scala.concurrent.ExecutionContext.Implicits.global


def run(){
    done = false
    Future(doUpdate())
}

:silent



