import BIDMach.networks.layers._

val datadir = "../../data/CIFAR100/parts/"
val trainfname = datadir + "trainNCHW%d.fmat.lz4";
val labelsfname = datadir + "labels%d.imat.lz4";
val testfname = datadir + "testNCHW%d.fmat.lz4";
val testlabelsfname = datadir + "testlabels%d.imat.lz4";
val predsfname = datadir + "preds%d.fmat.lz4";

class MyOpts extends Learner.Options with Net.Opts with FileSource.Opts with Grad.Opts;
val opts = new MyOpts;
val ds = FileSource(trainfname, labelsfname, opts);
val net = new Net(opts);
val grad = new Grad(opts);
val nn = new Learner(ds, net, null, grad, null, opts);

val convt = jcuda.jcudnn.cudnnConvolutionMode.CUDNN_CROSS_CORRELATION

val lrinit = 1e-2f;

def lr_update(ipass:Float, istep:Float, frac:Float):Float = {
  if (ipass < 10) {
	lrinit;
  } else if (ipass < 20) {
	lrinit/10;
  } else {
	lrinit/100;
  }
}


opts.batchSize= 100;
opts.npasses = 20
opts.lrate = 1e-3f 

opts.vel_decay = 0.9f
//opts.gsq_decay = 0.99f
opts.texp = 0.0f
opts.pstep = 0.1f
opts.hasBias = true;
opts.l2reg = 0.0005f;
opts.lr_policy = lr_update _;
opts.tensorFormat = Net.TensorNCHW;
opts.autoReset = false;
opts.debugMem = true;


{
import BIDMach.networks.layers.Node._;

Net.initDefaultNodeSet;

val in = input();
val scalef = constant(row(0.01f))();
val inscale = in *@ scalef

val conv1 = conv(inscale)(w=5,h=5,nch=32,stride=1,pad=0,initv=1f,convType=convt);
val pool1 = pool(conv1)(w=2,h=2,stride=2);
val norm1 = batchNormScale(pool1)();
val relu1 = relu(norm1)();
//val relu1 = relu(pool1)();

val conv2 = conv(relu1)(w=5,h=5,nch=32,stride=1,pad=0,convType=convt);
val pool2 = pool(conv2)(w=2,h=2,stride=2);
val norm2 = batchNormScale(pool2)();
val relu2 = relu(norm2)();
//val relu2 = relu(pool2)();

val conv3 = conv(relu2)(w=5,h=5,nch=32,stride=1,pad=2,convType=convt);
val pool3 = pool(conv3)(w=3,h=3,stride=2);
val fc3 =   linear(pool3)(outdim=100,initv=3e-2f);
val out =   softmaxout(fc3)(scoreType=1); 

opts.nodeset=Net.getDefaultNodeSet
}

val model = nn.model.asInstanceOf[Net];

def loss = {net.layers(net.layers.length-1).asInstanceOf[SoftmaxOutputLayer]};

val sgd = nn.updater.asInstanceOf[Grad];

nn.train;


val (mm, mopts) =  Net.predictor(model, testfname, testlabelsfname, predsfname);
mopts.autoReset = false;
mopts.batchSize = 100;
val mmodel = mm.model.asInstanceOf[Net];
mm.predict;

println("Accuracy = %f" format mean(mm.results(0,?),2).v);


