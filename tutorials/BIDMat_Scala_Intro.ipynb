{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to BIDMat and Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIDMat is a multi-platform matrix library similar to R, Matlab, Julia or Numpy/Scipy. It takes full advantage of the very powerful Scala Language. Its intended primarily for machine learning, but is has a broad set of operations and datatypes and should be suitable for many other applications. BIDMat has several unique features:\n",
    "\n",
    "* Built from the ground up with GPU + CPU backends. BIDMat code is implementation independent. \n",
    "* GPU memory management uses caching, designed to support iterative algorithms.\n",
    "* Natural and extensible syntax (thanks to scala). Math operators include +,-,*,/,⊗,∙,∘\n",
    "* Probably the most complete support for matrix types: dense matrices of float32, double, int and long. Sparse matrices with single or double elements. All are available on CPU or GPU. \n",
    "* Highest performance sparse matrix operations on power-law data. \n",
    "\n",
    "BIDMat has several other state-of-the-art features:\n",
    "* Interactivity. Thanks to the Scala language, BIDMat is interactive and scriptable. \n",
    "* Massive code base thanks to Java. \n",
    "* Easy-to-use Parallelism, thanks to Scala's actor framework and parallel collection classes.  \n",
    "* Runs on JVM, extremely portable. Runs on Mac, Linux, Windows, Android.\n",
    "* Cluster-ready, leverages Hadoop, Yarn, Spark etc. \n",
    "\n",
    "BIDMat is a library that is loaded by a startup script, and a set of imports that include the default classes and functions. We include them explicitly in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CUDA device found, CUDA version 7.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99132067,11974557696,12079398912)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import BIDMat.{CMat,CSMat,DMat,Dict,IDict,FMat,FND,GMat,GDMat,GIMat,GLMat,GSMat,GSDMat,\n",
    "               HMat,IMat,Image,LMat,Mat,ND,SMat,SBMat,SDMat}\n",
    "import BIDMat.MatFunctions._\n",
    "import BIDMat.SciFunctions._\n",
    "import BIDMat.Solvers._\n",
    "import BIDMat.Plotting._\n",
    "\n",
    "Mat.checkMKL\n",
    "Mat.checkCUDA\n",
    "Mat.inline = true\n",
    "if (Mat.hasCUDA > 0) GPUmem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These calls check that CPU and GPU native libs loaded correctly, and what GPUs are accessible.\n",
    "If you have a GPU and CUDA installed, GPUmem will printout the fraction of free memory, the absolute free memory and the total memory for the default GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU and GPU matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIDMat's matrix types are given in the table below. All are children of the \"Mat\" parent class, which allows code to be written generically. Many of BIDMach's learning algorithms will run with either single or double precision, dense or sparse input data. \n",
    "<table style=\"width:4in\" align=\"left\">\n",
    "<tr><td/><td colspan=\"2\"><b>CPU Matrices</b></td><td colspan=\"2\"><b>GPU Matrices</b></td></tr>\n",
    "<tr><td></td><td><b>Dense</b></td><td><b>Sparse</b></td><td><b>Dense</b></td><td><b>Sparse</b></td></tr>\n",
    "<tr><td><b>Float32</b></td><td>FMat</td><td>SMat</td><td>GMat</td><td>GSMat</td></tr>\n",
    "<tr><td><b>Float64</b></td><td>DMat</td><td>SDMat</td><td>GDMat</td><td>GSDMat</td></tr>\n",
    "<tr><td><b>Int32</b></td><td>IMat</td><td></td><td>GIMat</td><td></td></tr>\n",
    "<tr><td><b>Int64</b></td><td>LMat</td><td></td><td>GLMat</td><td></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val n = 4096          // \"val\" designates a constant. n is statically typed (as in Int here), but its type is inferred.\n",
    "val a = rand(n,n)     // Create an nxn matrix (on the CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%type a             // Most scientific funtions in BIDMat return single-precision results by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU matrix operations use Intel MKL acceleration for linear algebra, scientific and statistical functions. BIDMat includes \"tic\" and \"toc\" for timing, and \"flip\" and \"flop\" for floating point performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flip; val b = a * a; val gf=gflop\n",
    "print(\"The product took %4.2f seconds at %3.0f gflops\" format (gf._2, gf._1))\n",
    "gf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "GPU matrices behave very similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ga = grand(n,n)            // Another nxn random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flip; val gb = ga * ga; val gf=gflop\n",
    "print(\"The product took %4.2f seconds at %3.0f gflops\" format (gf._2, gf._1))\n",
    "gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%type ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But much of the power of BIDMat is that we dont have to worry about matrix types. Lets explore that with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD (Singular Value Decomposition) on a Budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try solving a real problem with this infrastructure: An approximate Singular-Value Decomposition (SVD) or PCA of a matrix M. We'll do this by computing the leading eigenvalues and eigenvectors of M x M^t. The method we use is **subspace iteration** and it generalizes the *power method* for computing the largest-magnitude eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SVD(M:Mat, ndims:Int, niter:Int) = {\n",
    "    var Q = M.zeros(M.nrows, ndims)                 // A block of ndims column vectors\n",
    "    normrnd(0, 1, Q)                                // randomly initialize the vectors\n",
    "    Mat.useCache = true                             // Turn matrix caching on\n",
    "    \n",
    "    for (i <- 0 until niter) {                      // Perform subspace iteration\n",
    "        val P = (Q.t * M *^ M).t                    // Compute P = M * M^t * Q efficiently\n",
    "        QRdecompt(P, Q, null)                       // QR-decomposition of P, saving Q\n",
    "    }\n",
    "    Mat.useCache = false                            // Turn caching off after the iteration\n",
    "    val P = (Q.t * M *^ M).t                        // Compute P again.\n",
    "    (Q, getdiag(P ^* Q))                            // Return Left singular vectors and singular values\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the code above used only the \"Mat\" matrix type. If you examine the variables V and P in a Scala IDE (Eclipse has one) you will find that they both also have type \"Mat\". Let's try it with an FMat (CPU single precision, dense matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Data Example\n",
    "\n",
    "We load some data from the MovieLens project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ndims = 32                             // Number of PCA dimension\n",
    "val niter = 128                            // Number of iterations to do\n",
    "\n",
    "val S = loadSMat(\"../data/movielens/train.smat.lz4\")(0->10000,0->4000)\n",
    "val M = full(S)                            // Put in a dense matrix\n",
    "\n",
    "flip; \n",
    "val (svecs, svals) = SVD(M, ndims, niter); // Compute the singular vectors and values\n",
    "val gf=gflop\n",
    "\n",
    "print(\"The calculation took %4.2f seconds at %2.1f gflops\" format (gf._2, gf._1))\n",
    "svals.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the singular values on a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(svals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglog(row(1 to svals.length), svals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try it with a GPU, single-precision, dense matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val G = GMat(M)                            // Try a dense GPU matrix\n",
    "\n",
    "flip; \n",
    "val (svecs, svals) = SVD(G, ndims, niter); // Compute the singular vectors and values \n",
    "val gf=gflop\n",
    "\n",
    "print(\"The calculation took %4.2f seconds at %2.1f gflops\" format (gf._2, gf._1))\n",
    "svals.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not bad, the GPU version was nearly 5x faster. Now lets try a sparse, CPU single-precision matrix. Note that by construction our matrix was only 10% dense anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flip;                                       // Try a sparse CPU matrix\n",
    "\n",
    "val (svecs, svals) = SVD(S, ndims, niter);  // Compute the singular vectors and values\n",
    "val gf=gflop    \n",
    "\n",
    "print(\"The calculation took %4.2f seconds at %2.1f gflops\" format (gf._2, gf._1))\n",
    "svals.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one is important. Dense matrix operations are the bread-and-butter of scientific computing, and now most deep learning. But other machine learning tasks (logistic regression, SVMs, k-Means, topic models etc) most commonly take *sparse* input data like text, URLs, cookies etc. And so performance on sparse matrix operations is critical. \n",
    "\n",
    "GPU performance on sparse data, especially *power law* data - which covers most of the case above (the commerically important cases) - has historically been poor. But in fact GPU hardware supports **extremely fast** sparse operations when the kernels are carefully designed. Such kernels are only available in BIDMat right now. NVIDIA's sparse matrix kernels, which have been tuned for sparse scientific data, do not work well on power-law data. \n",
    "\n",
    "In any case, let's try BIDMat's GPU sparse matrix type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val GS = GSMat(S)                           // Try a sparse GPU matrix\n",
    "\n",
    "flip;\n",
    "val (svecs, svals) = SVD(GS, ndims, niter); // Compute the singular vectors and values\n",
    "val gf=gflop\n",
    "\n",
    "print(\"The calculation took %4.2f seconds at %2.1f gflops\" format (gf._2, gf._1))\n",
    "svals.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a 10x improvement end-to-end, which is similar to the GPU's advantage on dense matrices. This result is certainly not specific to SVD, and is reproduced in most ML algorithms. So GPUs have a key role to play in general machine learning, and its likely that at some point they will assume a central role as they currently enjoy in scientific computing and deep learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Double Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last performance issue: GPU hardware normally prioritizes single-precision floating point over double-precision, and there is a big gap on dense matrix operations. But calculations on sparse data are memory-limited and this largely masks the difference in arithmetic. Lets try a sparse, double-precision matrix, which will force all the calculations to double precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val GSD = GSDMat(GS)                             // Try a sparse, double GPU matrix\n",
    "\n",
    "flip; \n",
    "val (svecs, svals) = SVD(GSD, ndims, niter); // Compute the singular vectors and values\n",
    "val gf=gflop\n",
    "\n",
    "print(\"The calculation took %4.2f seconds at %2.1f gflops\" format (gf._2, gf._1))\n",
    "svals.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is noticebly slower, but still 3x faster than the CPU version running in single precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cusparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA's cusparse library, which is optimized for scientific data, doesnt perform as well on power-law data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SVD(M:Mat, ndims:Int, niter:Int) = {\n",
    "    var Q = M.zeros(M.nrows, ndims)\n",
    "    normrnd(0, 1, Q)\n",
    "    Mat.useCache = true     \n",
    "    for (i <- 0 until niter) {                      // Perform subspace iteration\n",
    "        val P = M * (M ^* Q)                        // Compute P = M * M^t * Q with cusparse\n",
    "        QRdecompt(P, Q, null) \n",
    "    }\n",
    "    Mat.useCache = false \n",
    "    val P = M * (M ^* Q)                            // Compute P again.\n",
    "    (Q, getdiag(P ^* Q))                            // Left singular vectors and singular values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                                                   // Try sparse GPU matrix\n",
    "flip; \n",
    "val (svecs, svals) = SVD(GS, ndims, niter); \n",
    "val gf=gflop\n",
    "\n",
    "print(\"The calculation took %4.2f seconds at %2.1f gflops\" format (gf._2, gf._1))\n",
    "svals.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Math Operators and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the standard operators +,-,*,/, BIDMat includes several other important operators with their standard unicode representation. They have an ASCII alias in case unicode input is difficult. Here they are:\n",
    "\n",
    "<pre>\n",
    "Unicode operator    ASCII alias    Operation\n",
    "================    ===========    =========\n",
    "       ∘                *@         Element-wise (Hadamard) product\n",
    "       ∙                dot        Column-wise dot product\n",
    "       ∙→              dotr        Row-wise dot product\n",
    "       ⊗               kron        Kronecker (Cartesian) product\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val a = ones(4,1) * row(1->5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val b = col(1->5) * ones(1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadamard (element-wise) multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b ∘ a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product, by default along columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b ∙ a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b ∙→ a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kronecker product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b ⊗ a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as operators, functions in BIDMach can use unicode characters. e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ii = row(1->10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ii on Γ(ii)                            // Stack this row on the results of a Gamma function applied to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed Multiplies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiply is the most expensive step in many calculations, and often involves transposed matrices. To speed up those calcualtions, we expose two operators that combine the transpose and multiply operations:\n",
    "\n",
    "<pre>\n",
    "^&ast;  - transpose the first argument, so a ^&ast; b is equivalent to a.t &ast; b\n",
    "&ast;^  - transpose the second argument, so a &ast;^ b is equivalent to a &ast; b.t\n",
    "</pre>\n",
    "these operators are implemented natively, i.e. they do not actually perform transposes, but implement the effective calculation. This is particulary important for sparse matrices since transpose would involve an index sort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a ^* b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.t * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a *^ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a * b.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights of the Scala Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala is a remarkable language. It is an object-oriented language with similar semantics to Java which it effectively extends. But it also has a particular clean functional syntax for anonymous functions and closures. \n",
    "\n",
    "It has a REPL (Read-Eval-Print-Loop) like Python, and can be used interactively or it can run scripts in or outside an interactive session. \n",
    "\n",
    "Like Python, types are determined by assignments, but they are static rather than dynamic. So the language has the economy of Python, but the type-safety of a static language. \n",
    "\n",
    "Scala includes a tuple type for multiple-value returns, and on-the-fly data structuring. \n",
    "\n",
    "Finally it has outstanding support for concurrency with parallel classes and an **actor** system called Akka. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we examine the performance of Scala as a scientific language. Let's implement an example that has been widely used to illustrate the performance of the Julia language. Its a random walk, i.e. a 1D array with random steps from one element to the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import java.util.Random\n",
    "val random = new Random()\n",
    "\n",
    "def rwalk(m:FMat) = {\n",
    "    val n = m.length\n",
    "    m(0) = random.nextFloat\n",
    "    var i = 1\n",
    "    while (i < n) {\n",
    "        m(i) = m(i-1) + random.nextFloat - 0.5f\n",
    "        i += 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val n = 100000000\n",
    "val a = zeros(n, 1)\n",
    "tic; val x = rwalk(a); val t=toc\n",
    "print(\"computed %2.1f million steps per second in %2.1f seconds\" format (n/t/1e6f, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try the same calculation in the Julia language (a new language designed for scientific computing) and in Python we find that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:4in\" align=\"left\">\n",
    "<tr><td></td><td><b>Scala</b></td><td><b>Julia</b></td><td><b>Python</b></td></tr>\n",
    "<tr><td><b>with rand</b></td><td>1.0s</td><td>0.43s</td><td>147s</td></tr>\n",
    "<tr><td><b>without rand</b></td><td>0.1s</td><td>0.26s</td><td>100s</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But does this matter? A random walk can be computed efficiently with vector operations: vector random numbers and a cumulative sum. And in general most ML algorithms can be implemented with vector and matrix operations efficiently. Let's try in BIDMat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic; rand(a); val b=cumsum(a-0.5f); val t=toc\n",
    "print(\"computed %2.1f million steps per second in %2.1f seconds\" format (n/t/1e6f, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is better, due to the faster random number generation in the vectorized rand function. But More interesting is the GPU running time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ga = GMat(a)\n",
    "tic; rand(ga); val gb=cumsum(ga-0.5f); val t=toc\n",
    "print(\"computed %2.1f million steps per second in %2.1f seconds\" format (n/t/1e6f, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run similar operators in Julia and Python we find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:5in\" align=\"left\">\n",
    "<tr><td></td><td><b>BIDMach(CPU)</b></td><td><b>BIDMach(GPU)</b></td><td><b>Julia</b></td><td><b>Python</b></td></tr>\n",
    "<tr><td><b>with rand</b></td><td>0.6s</td><td>0.1s</td><td>0.44s</td><td>1.4s</td></tr>\n",
    "<tr><td><b>without rand</b></td><td>0.3s</td><td>0.05s</td><td>0.26s</td><td>0.5s</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized operators even the playing field, and bring Python up to speed compared to the other systems. On the other hand, GPU hardware maintains a near-order-of-magnitude advantage for vector operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tapping the Java Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/html": [
       "<img style=\"width:4in\" alt=\"NGC 4414 (NASA-med).jpg\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/NGC_4414_%28NASA-med%29.jpg/1200px-NGC_4414_%28NASA-med%29.jpg\"/>"
      ],
      "text/plain": [
       "<img style=\"width:4in\" alt=\"NGC 4414 (NASA-med).jpg\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/NGC_4414_%28NASA-med%29.jpg/1200px-NGC_4414_%28NASA-med%29.jpg\"/>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<img style=\"width:4in\" alt=\"NGC 4414 (NASA-med).jpg\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/NGC_4414_%28NASA-med%29.jpg/1200px-NGC_4414_%28NASA-med%29.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost every piece of Java code can be used in Scala. And therefore any piece of Java code can be used interactively. \n",
    "\n",
    "There's very little work to do. You find a package and add it to your dependencies and then import as you would in Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.commons.math3.stat.inference.TestUtils._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Commons Math includes a Statistics package with many useful functions and tests. Lets create two arrays of random data and compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val x = normrnd(0,1,1,40)\n",
    "val y = normrnd(0,1,1,40) + 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIDMat has enriched matrix types like FMat, SMat etc, while Apache Commons Math expects Java Arrays of Double precision floats. To get these, we can convert FMat to DMat (double) and extra the data field which contains the matrices data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dx = DMat(a)\n",
    "val dy = DMat(b)\n",
    "tTest(da.data, db.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But rather than doing this conversion every time we want to use some BIDMat matrices, we can instruct Scala to do the work for us. We do this with an **implicit conversion** from FMat to Array[Double]. Simply defining this function will case a coercion whenever we supply an FMat argument to a function that expects Array[Double]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "implicit def fMatToDarray(a:FMat):Array[Double] = DMat(a).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And magically we can perform t-Tests on BIDMat matrices as though they had known each other all along. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tTest(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import org.apache.commons.math3.distribution._\n",
    "\n",
    "val betadist = new BetaDistribution(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "org.apache.commons.math3.distribution.BetaDistribution@4d1d2921"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n = 100000\n",
    "val x = new DMat(1, n, (0 until n).map(x => betadist.sample).toArray); null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "BufferedImage@1829e505: type = 2 DirectColorModel: rmask=ff0000 gmask=ff00 bmask=ff amask=ff000000 IntegerInterleavedRaster: width = 500 height = 300 #Bands = 4 xOff = 0 yOff = 0 dataOffset[0] 0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist(x, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/html": [
       "<image src=\"https://sketchesfromthealbum.files.wordpress.com/2015/01/jacquesderrida.jpg\" style=\"width:4in\"/>"
      ],
      "text/plain": [
       "<image src=\"https://sketchesfromthealbum.files.wordpress.com/2015/01/jacquesderrida.jpg\" style=\"width:4in\"/>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "<image src=\"https://sketchesfromthealbum.files.wordpress.com/2015/01/jacquesderrida.jpg\" style=\"width:4in\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a raw Java Array of float integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val i = row(0->10).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, Scala supports Tuple types for ad-hoc data structuring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val j = i.map(x => (x, x*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also **deconstruct** tuples using Scala Pattern matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j.map{case (x,y) => (y,x)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And reduce operations can use deconstruction as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val k = j.reduce((ab,cd) => {val (a,b) = ab; val (c,d) = cd; (a+c, b+d)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
