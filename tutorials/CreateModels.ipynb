{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main goals of BIDMat/BIDMach is to make model creation, customization and experimentation much easier. \n",
    "\n",
    "To that end is has reusable classes that cover the elements of Learning:\n",
    "\n",
    "* Model: The core class for a learning algorithm, and often the only one you need to implement.\n",
    "* DataSource: A source of data, like an in-memory matrix, a set of files (possibly on HDFS) or a data iterator (for Spark).\n",
    "* DataSink: A target for data such as predictions, like an in-memory matrix, a set of files, or an iterator. \n",
    "* Updaters: Update a model using minibatch update from a Model class. Includes SGD, ADAGRAD, Monte-Carlo updates, and multiplicative updates. \n",
    "* Mixins: Secondary Loss functions that are added to the global gradient. Includes L1 and L2 regularizers, cluster quality metrics, factor model metrics. \n",
    "* Learner: Combines the classes above and provides high-level control over the learning process: iterations, stop/start/resume\n",
    "\n",
    "When creating a new model, its often only necessary to creat a new model class. We recently needed a scalable SVD (Singular Value Decomposition) for some student projects. Lets walk through creating this from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable SVD\n",
    "\n",
    "This model works like the previous example of in-memory SVD for a matrix M. The singular values of M are the eigenvalues of M M^T so we do subspace iteration: \n",
    "\n",
    "$$P = M M^T Q$$\n",
    "$$(Q,R) = QR(P)$$\n",
    "\n",
    "But now we want to deal with an M which is too big to fit in memory. In the minibatch context, we can write M as a horizontal concatenation of mini-batches (this assumes data samples are columns of M and features are rows):\n",
    "\n",
    "$$M = M_1 M_2 \\cdots M_n$$\n",
    "\n",
    "and then $$P = \\sum_{i=1}^n M_i M_i^T Q$$\n",
    "\n",
    "so we can compute $P$ by operating only on the minibatches $M_i$. We need to be able to fit $P$ and $Q$ in memory, their size is only $k~ |F|$ where $k$ is the SVD dimension and $F$ is the feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a new model class which extends BIDMach's Model class. It will always take an Options instance as an argument:\n",
    "\n",
    "<b>\n",
    "<code style=\"color:blue\">\n",
    "class SVD(opts:SVD.Opts = new SVD.Options) extends Model(opts)\n",
    "</code>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options are defined in the \"Object\" associated with the class. In Scala \"Object\" defines a singleton which holds all of the static methods of the class. It looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><code style=\"color:blue\">\n",
    "object SVD  {\n",
    "  trait Opts extends Model.Opts {\n",
    "    var deliciousness = 3\n",
    "  }\n",
    "  \n",
    "  class Options extends Opts {}\n",
    "  ...\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truthfully, an SVD model doesnt need a \"deliciousness\" option, in fact it doesnt need any Options at all - or rather what it needs is inherited from its parent. But its there to show how options are created. The Opts are defined as a trait rather than a class so they can be mixed in with the Options of other learning classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Variables and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three variables we need to keep track of:\n",
    "\n",
    "<b><code style=\"color:blue\">\n",
    "  var Q:Mat = null;                                        // (Left) Singular vectors\n",
    "  var SV:Mat = null;                                       // Singular values\n",
    "  var P:Mat = null;                                        // P (accumulator)\n",
    "</code></b>\n",
    "\n",
    "and an initialization routine sets these to appropriate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each update should update the stable model: Here its $P$:\n",
    "\n",
    "<b><code style=\"color:blue\">\n",
    "  def dobatch(mats:Array[Mat], ipass:Int, pos:Long):Unit = {\n",
    "    val M = mats(0);\n",
    "    P ~ P + (Q.t &ast; M &ast;&#94; M).t                               // Compute P = M &ast; M&#94;t &ast; Q efficiently\n",
    "  }\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score method should return a floating point vector of scores for this minibatch.\n",
    "\n",
    "<b><code style=\"color:blue\">\n",
    "  def evalbatch(mat:Array[Mat], ipass:Int, pos:Long):FMat = {\n",
    "    SV ~ P âˆ™ Q;                                            // Estimate the singular values\n",
    "    val diff = (P / SV) - Q;                               // residual\n",
    "    row(-(math.sqrt(norm(diff) / diff.length)));           // return the norm of the residual\n",
    "  }\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of a pass over the data, we update $Q$. Not all models need this step, and minibatch algorithms typically dont have it. \n",
    "\n",
    "<b><code style=\"color:blue\">\n",
    "  override def updatePass(ipass:Int) = {   \n",
    "    QRdecompt(P, Q, null);                                 // Basic subspace iteration\n",
    "    P.clear;                                               // Clear P for the next pass\n",
    "  }\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done defining the SVD model. We can run it now, but to make that easier we'll define a couple of convenience functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An in-memory Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><code style=\"color:blue\">\n",
    "  class MatOptions extends Learner.Options with SVD.Opts with MatSource.Opts with Batch.Opts\n",
    "  \n",
    "  def learner(mat:Mat):(Learner, MatOptions) = { \n",
    "    val opts = new MatOptions;\n",
    "    opts.batchSize = math.min(100000, mat.ncols/30 + 1)\n",
    "  \tval nn = new Learner(\n",
    "  \t    new MatSource(Array(mat), opts), \n",
    "  \t\t\tnew SVD(opts), \n",
    "  \t\t\tnull,\n",
    "  \t\t\tnew Batch(opts), \n",
    "  \t\t\tnull,\n",
    "  \t\t\topts)\n",
    "    (nn, opts)\n",
    "  }\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A File-based Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><code style=\"color:blue\">\n",
    "  class FileOptions extends Learner.Options with SVD.Opts with FileSource.Opts with Batch.Opts\n",
    "  \n",
    "  def learner(fnames:String):(Learner, FileOptions) = { \n",
    "    val opts = new FileOptions;\n",
    "    opts.batchSize = 10000;\n",
    "    opts.fnames = List(FileSource.simpleEnum(fnames, 1, 0));\n",
    "    implicit val threads = threadPool(4);\n",
    "  \tval nn = new Learner(\n",
    "  \t    new FileSource(opts), \n",
    "  \t\t\tnew SVD(opts), \n",
    "  \t\t\tnull,\n",
    "  \t\t\tnew Batch(opts), \n",
    "  \t\t\tnull,\n",
    "  \t\t\topts)\n",
    "    (nn, opts)\n",
    "  }\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A predictor is a Learner which runs an existing model over a DataSource and outputs to a DataSink. For SVD, the predictor outputs the right singular vectors, which may be too large to fit in memory. Here's a memory-to-memory predictor:\n",
    "\n",
    "<b><code style=\"color:blue\">\n",
    " class PredOptions extends Learner.Options with SVD.Opts with MatSource.Opts with MatSink.Opts;\n",
    "  \n",
    "  // This function constructs a predictor from an existing model \n",
    "  def predictor(model:Model, mat1:Mat):(Learner, PredOptions) = {\n",
    "    val nopts = new PredOptions;\n",
    "    nopts.batchSize = math.min(10000, mat1.ncols/30 + 1)\n",
    "    nopts.dim = model.opts.dim;\n",
    "    val newmod = new SVD(nopts);\n",
    "    newmod.refresh = false\n",
    "    model.copyTo(newmod)\n",
    "    val nn = new Learner(\n",
    "        new MatSource(Array(mat1), nopts), \n",
    "        newmod, \n",
    "        null,\n",
    "        null,\n",
    "        new MatSink(nopts),\n",
    "        nopts)\n",
    "    (nn, nopts)\n",
    "  }\n",
    "</code></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try it out! First we initialize BIDMach as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $exec.^.lib.bidmach_notebook_init\n",
    "if (Mat.hasCUDA > 0) GPUmem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run on the MNIST 8M (8 millon images) digit data, which is a large dataset distributed over multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dir=\"../data/MNIST8M/parts/\"\n",
    "val (nn, opts) = SVD.learner(dir+\"data%02d.fmat.lz4\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set some options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.nend = 10;\n",
    "opts.dim = 20;\n",
    "opts.npasses = 2;\n",
    "opts.batchSize = 20000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and release the beast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model matrices for this model hold the results. They are generic matrices, so we cast them to FMats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val svals = FMat(nn.modelmats(1));\n",
    "val svecs = FMat(nn.modelmats(0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semilogy(svals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well we did, we will compute the SVD directly by computing $M M^T$ and computing its eigendecomposition. Normally we can't do this because $MM^T$ is nfeats x nfeats, but for this dataset nfeats is only 784. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic\n",
    "val MMt = zeros(784,784);\n",
    "for (i <- 0 until opts.nend) {\n",
    "val Mi = loadFMat(dir+\"data%02d.fmat.lz4\" format i);\n",
    "MMt ~ Mi *^ Mi;\n",
    "print(\".\");\n",
    "} \n",
    "println;\n",
    "toc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call an eigenvalue routine to compute the eigenvalues and eigenvectors of $MM^T$, which are the singular values and left singular vectors of $M$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (eval, evecs) = feig(MMt);\n",
    "val topvecs = evecs(?, 783 to 784-opts.dim by -1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors have a sign ambiguity, and its common to see V or -V. So next we compute dot products between the two sets of vectors and flip signs if a dot product is negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dots = svecs âˆ™ topvecs;\n",
    "svecs ~ svecs âˆ˜ (2*(dots>0) - 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now look at the eigenvectors as small images, decreasing in strength from left to right. First the reference eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val onerow = topvecs.view(28,28*opts.dim);\n",
    "val nc = onerow.ncols;\n",
    "val tworows = onerow(?,0->(nc/2)) on onerow(?,(nc/2)->nc)\n",
    "show((tworows.t*500+128) âŠ— ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val onerow = svecs.view(28,28*opts.dim);\n",
    "val nc = onerow.ncols;\n",
    "val tworows = onerow(?,0->(nc/2)) on onerow(?,(nc/2)->nc)\n",
    "show((tworows.t*500+128) âŠ— ones(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Right Singular Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we obtained the singular values and left singular vectors from the model's modelmats array. The right singular vectors grow in size with the dataset, and in general wont fit in memory. But we can still compute them by running a predictor on the dataset. This predictor takes a parametrized input file name for the matrix $M$ and a parametrized output file name to hold the right singular vectors. A key option to set is <code>ofcols</code> the number of samples per output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (pp, popts) = SVD.predictor(nn.model, dir+\"data%02d.fmat.lz4\", dir+\"rSingVectors%02d.fmat.lz4\")\n",
    "popts.ofcols = 100000                  // number of columns per file, here the same as the input files\n",
    "popts.nend = 10                        // Number of input files to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BIDMach's actual SVD model is only slightly different from the code presented here: First it computes the subspace updates on *minibatches* of data for the first few iterations to more rapidly get near a good model. Secondly it alternates subspace iteration with Rayleigh-Ritz iterations for faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
